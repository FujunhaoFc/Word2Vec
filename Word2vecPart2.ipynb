{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FujunhaoFc/Word2Vec/blob/main/Word2vecPart2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTNxpLPQJB_9"
      },
      "source": [
        "# Part 2 Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hdqWJKUtPusL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5TeD4nuPuuq",
        "outputId": "db3cd7bc-f91e-43e7-a070-42a44ddda785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read data from files\n",
        "train = pd.read_csv( \"word2vec/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "test = pd.read_csv( \"word2vec/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "unlabeled_train = pd.read_csv( \"word2vec/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
        "\n",
        "# Verify the number of reviews that were read (100,000 in total)\n",
        "print (\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
        " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,\n",
        " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "c5OKZ1nLPuxT",
        "outputId": "b8a30945-7944-4210-8301-05aed562eb04"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"5814_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"With all this stuff going down at the moment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"2381_9\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"7759_3\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"3630_4\"</td>\n",
              "      <td>0</td>\n",
              "      <td>\"It must be assumed that those who praised thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"9495_8\"</td>\n",
              "      <td>1</td>\n",
              "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  sentiment                                             review\n",
              "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
              "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
              "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
              "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
              "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "N128AdozPuzl",
        "outputId": "7188569a-5ec2-4daf-cec4-b66885d26420"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"12311_10\"</td>\n",
              "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"8348_2\"</td>\n",
              "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"5828_4\"</td>\n",
              "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7186_2\"</td>\n",
              "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"12128_7\"</td>\n",
              "      <td>\"A very accurate depiction of small time mob l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id                                             review\n",
              "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
              "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
              "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
              "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
              "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xHC3MBB9Pu2F",
        "outputId": "2e0ce5df-6ff9-4072-f0cb-42f437bce279"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"9999_0\"</td>\n",
              "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"45057_0\"</td>\n",
              "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"15561_0\"</td>\n",
              "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7161_0\"</td>\n",
              "      <td>\"I went to see this film with a great deal of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"43971_0\"</td>\n",
              "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                             review\n",
              "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
              "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
              "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
              "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
              "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unlabeled_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_FSvB6a2cYPG"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a8DW94fcdAh8"
      },
      "outputs": [],
      "source": [
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words.  Returns a list of words.\n",
        "    #\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "    #\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    #\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    #\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    #\n",
        "    # 5. Return a list of words\n",
        "    return(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9gF6RsqwlYw0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/junhaofu/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/junhaofu/nltk_data', '/Users/junhaofu/ENTER/nltk_data', '/Users/junhaofu/ENTER/share/nltk_data', '/Users/junhaofu/ENTER/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "print(nltk.data.path)\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # Function to split a review into parsed sentences. Returns a\n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    #\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
        "              remove_stopwords ))\n",
        "    #\n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRKh5PyklYzV",
        "outputId": "7264a9dd-dbd1-4589-bf6b-93bb94b148ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z5/5fqf9f4x2f9ddt98nd465jh00000gn/T/ipykernel_38956/2861317490.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  review_text = BeautifulSoup(review).get_text()\n",
            "/var/folders/z5/5fqf9f4x2f9ddt98nd465jh00000gn/T/ipykernel_38956/2861317490.py:6: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  review_text = BeautifulSoup(review).get_text()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing sentences from unlabeled set\n"
          ]
        }
      ],
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print(\"Parsing sentences from training set\")\n",
        "for review in train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "\n",
        "print(\"Parsing sentences from unlabeled set\")\n",
        "for review in unlabeled_train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekWA2gxPlY1-",
        "outputId": "acda06cc-7fb1-450b-b7e8-1cfd187d7db2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['with',\n",
              " 'all',\n",
              " 'this',\n",
              " 'stuff',\n",
              " 'going',\n",
              " 'down',\n",
              " 'at',\n",
              " 'the',\n",
              " 'moment',\n",
              " 'with',\n",
              " 'mj',\n",
              " 'i',\n",
              " 've',\n",
              " 'started',\n",
              " 'listening',\n",
              " 'to',\n",
              " 'his',\n",
              " 'music',\n",
              " 'watching',\n",
              " 'the',\n",
              " 'odd',\n",
              " 'documentary',\n",
              " 'here',\n",
              " 'and',\n",
              " 'there',\n",
              " 'watched',\n",
              " 'the',\n",
              " 'wiz',\n",
              " 'and',\n",
              " 'watched',\n",
              " 'moonwalker',\n",
              " 'again']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrIAxO4JlY4P",
        "outputId": "22e80a90-006c-4a33-b0a9-b884cab66844"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['maybe',\n",
              " 'i',\n",
              " 'just',\n",
              " 'want',\n",
              " 'to',\n",
              " 'get',\n",
              " 'a',\n",
              " 'certain',\n",
              " 'insight',\n",
              " 'into',\n",
              " 'this',\n",
              " 'guy',\n",
              " 'who',\n",
              " 'i',\n",
              " 'thought',\n",
              " 'was',\n",
              " 'really',\n",
              " 'cool',\n",
              " 'in',\n",
              " 'the',\n",
              " 'eighties',\n",
              " 'just',\n",
              " 'to',\n",
              " 'maybe',\n",
              " 'make',\n",
              " 'up',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'whether',\n",
              " 'he',\n",
              " 'is',\n",
              " 'guilty',\n",
              " 'or',\n",
              " 'innocent']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TBCahWDqKFO",
        "outputId": "8eefcbb9-ffa8-44ba-f0eb-90edb2a51bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /Users/junhaofu/ENTER/lib/python3.9/site-packages (from gensim) (1.26.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/junhaofu/ENTER/lib/python3.9/site-packages (from gensim) (1.12.0)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: wrapt in /Users/junhaofu/ENTER/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-4.3.3 smart-open-7.3.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgw76W7KoFsU",
        "outputId": "88309bac-686f-4b87-8d81-9788909ec4a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-15 20:10:44,030 : INFO : collecting all words and their counts\n",
            "2025-10-15 20:10:44,030 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2025-10-15 20:10:44,076 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n",
            "2025-10-15 20:10:44,108 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n",
            "2025-10-15 20:10:44,137 : INFO : PROGRESS: at sentence #30000, processed 670859 words, keeping 30027 word types\n",
            "2025-10-15 20:10:44,168 : INFO : PROGRESS: at sentence #40000, processed 896841 words, keeping 34335 word types\n",
            "2025-10-15 20:10:44,202 : INFO : PROGRESS: at sentence #50000, processed 1116082 words, keeping 37751 word types\n",
            "2025-10-15 20:10:44,239 : INFO : PROGRESS: at sentence #60000, processed 1337544 words, keeping 40711 word types\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-15 20:10:44,271 : INFO : PROGRESS: at sentence #70000, processed 1560307 words, keeping 43311 word types\n",
            "2025-10-15 20:10:44,311 : INFO : PROGRESS: at sentence #80000, processed 1779516 words, keeping 45707 word types\n",
            "2025-10-15 20:10:44,353 : INFO : PROGRESS: at sentence #90000, processed 2003714 words, keeping 48121 word types\n",
            "2025-10-15 20:10:44,384 : INFO : PROGRESS: at sentence #100000, processed 2225465 words, keeping 50190 word types\n",
            "2025-10-15 20:10:44,417 : INFO : PROGRESS: at sentence #110000, processed 2444323 words, keeping 52058 word types\n",
            "2025-10-15 20:10:44,457 : INFO : PROGRESS: at sentence #120000, processed 2666488 words, keeping 54098 word types\n",
            "2025-10-15 20:10:44,493 : INFO : PROGRESS: at sentence #130000, processed 2892315 words, keeping 55837 word types\n",
            "2025-10-15 20:10:44,525 : INFO : PROGRESS: at sentence #140000, processed 3104796 words, keeping 57324 word types\n",
            "2025-10-15 20:10:44,557 : INFO : PROGRESS: at sentence #150000, processed 3330432 words, keeping 59045 word types\n",
            "2025-10-15 20:10:44,590 : INFO : PROGRESS: at sentence #160000, processed 3552466 words, keeping 60581 word types\n",
            "2025-10-15 20:10:44,623 : INFO : PROGRESS: at sentence #170000, processed 3776048 words, keeping 62050 word types\n",
            "2025-10-15 20:10:44,655 : INFO : PROGRESS: at sentence #180000, processed 3996237 words, keeping 63483 word types\n",
            "2025-10-15 20:10:44,693 : INFO : PROGRESS: at sentence #190000, processed 4221288 words, keeping 64775 word types\n",
            "2025-10-15 20:10:44,730 : INFO : PROGRESS: at sentence #200000, processed 4445973 words, keeping 66070 word types\n",
            "2025-10-15 20:10:44,766 : INFO : PROGRESS: at sentence #210000, processed 4666511 words, keeping 67367 word types\n",
            "2025-10-15 20:10:44,808 : INFO : PROGRESS: at sentence #220000, processed 4892037 words, keeping 68686 word types\n",
            "2025-10-15 20:10:44,838 : INFO : PROGRESS: at sentence #230000, processed 5113881 words, keeping 69935 word types\n",
            "2025-10-15 20:10:44,872 : INFO : PROGRESS: at sentence #240000, processed 5340847 words, keeping 71144 word types\n",
            "2025-10-15 20:10:44,902 : INFO : PROGRESS: at sentence #250000, processed 5555463 words, keeping 72333 word types\n",
            "2025-10-15 20:10:44,936 : INFO : PROGRESS: at sentence #260000, processed 5775304 words, keeping 73466 word types\n",
            "2025-10-15 20:10:44,970 : INFO : PROGRESS: at sentence #270000, processed 5995572 words, keeping 74740 word types\n",
            "2025-10-15 20:10:45,006 : INFO : PROGRESS: at sentence #280000, processed 6220911 words, keeping 76318 word types\n",
            "2025-10-15 20:10:45,051 : INFO : PROGRESS: at sentence #290000, processed 6443523 words, keeping 77787 word types\n",
            "2025-10-15 20:10:45,082 : INFO : PROGRESS: at sentence #300000, processed 6668258 words, keeping 79142 word types\n",
            "2025-10-15 20:10:45,117 : INFO : PROGRESS: at sentence #310000, processed 6892662 words, keeping 80431 word types\n",
            "2025-10-15 20:10:45,152 : INFO : PROGRESS: at sentence #320000, processed 7118969 words, keeping 81794 word types\n",
            "2025-10-15 20:10:45,184 : INFO : PROGRESS: at sentence #330000, processed 7340486 words, keeping 83006 word types\n",
            "2025-10-15 20:10:45,220 : INFO : PROGRESS: at sentence #340000, processed 7569986 words, keeping 84252 word types\n",
            "2025-10-15 20:10:45,255 : INFO : PROGRESS: at sentence #350000, processed 7792927 words, keeping 85407 word types\n",
            "2025-10-15 20:10:45,291 : INFO : PROGRESS: at sentence #360000, processed 8012526 words, keeping 86567 word types\n",
            "2025-10-15 20:10:45,336 : INFO : PROGRESS: at sentence #370000, processed 8239772 words, keeping 87663 word types\n",
            "2025-10-15 20:10:45,372 : INFO : PROGRESS: at sentence #380000, processed 8465827 words, keeping 88849 word types\n",
            "2025-10-15 20:10:45,408 : INFO : PROGRESS: at sentence #390000, processed 8694607 words, keeping 89883 word types\n",
            "2025-10-15 20:10:45,442 : INFO : PROGRESS: at sentence #400000, processed 8917820 words, keeping 90882 word types\n",
            "2025-10-15 20:10:45,474 : INFO : PROGRESS: at sentence #410000, processed 9138504 words, keeping 91859 word types\n",
            "2025-10-15 20:10:45,507 : INFO : PROGRESS: at sentence #420000, processed 9358474 words, keeping 92880 word types\n",
            "2025-10-15 20:10:45,540 : INFO : PROGRESS: at sentence #430000, processed 9586958 words, keeping 93909 word types\n",
            "2025-10-15 20:10:45,601 : INFO : PROGRESS: at sentence #440000, processed 9812576 words, keeping 94853 word types\n",
            "2025-10-15 20:10:45,632 : INFO : PROGRESS: at sentence #450000, processed 10036719 words, keeping 95995 word types\n",
            "2025-10-15 20:10:45,663 : INFO : PROGRESS: at sentence #460000, processed 10269931 words, keeping 97064 word types\n",
            "2025-10-15 20:10:45,694 : INFO : PROGRESS: at sentence #470000, processed 10496262 words, keeping 97885 word types\n",
            "2025-10-15 20:10:45,722 : INFO : PROGRESS: at sentence #480000, processed 10717170 words, keeping 98809 word types\n",
            "2025-10-15 20:10:45,753 : INFO : PROGRESS: at sentence #490000, processed 10943335 words, keeping 99835 word types\n",
            "2025-10-15 20:10:45,781 : INFO : PROGRESS: at sentence #500000, processed 11165141 words, keeping 100726 word types\n",
            "2025-10-15 20:10:45,810 : INFO : PROGRESS: at sentence #510000, processed 11390498 words, keeping 101672 word types\n",
            "2025-10-15 20:10:45,839 : INFO : PROGRESS: at sentence #520000, processed 11613511 words, keeping 102557 word types\n",
            "2025-10-15 20:10:45,869 : INFO : PROGRESS: at sentence #530000, processed 11838774 words, keeping 103374 word types\n",
            "2025-10-15 20:10:45,899 : INFO : PROGRESS: at sentence #540000, processed 12062185 words, keeping 104231 word types\n",
            "2025-10-15 20:10:45,930 : INFO : PROGRESS: at sentence #550000, processed 12286959 words, keeping 105098 word types\n",
            "2025-10-15 20:10:45,961 : INFO : PROGRESS: at sentence #560000, processed 12509034 words, keeping 105971 word types\n",
            "2025-10-15 20:10:45,991 : INFO : PROGRESS: at sentence #570000, processed 12736827 words, keeping 106757 word types\n",
            "2025-10-15 20:10:46,021 : INFO : PROGRESS: at sentence #580000, processed 12958427 words, keeping 107611 word types\n",
            "2025-10-15 20:10:46,053 : INFO : PROGRESS: at sentence #590000, processed 13184325 words, keeping 108468 word types\n",
            "2025-10-15 20:10:46,083 : INFO : PROGRESS: at sentence #600000, processed 13406551 words, keeping 109189 word types\n",
            "2025-10-15 20:10:46,113 : INFO : PROGRESS: at sentence #610000, processed 13628198 words, keeping 110055 word types\n",
            "2025-10-15 20:10:46,142 : INFO : PROGRESS: at sentence #620000, processed 13852588 words, keeping 110805 word types\n",
            "2025-10-15 20:10:46,171 : INFO : PROGRESS: at sentence #630000, processed 14075901 words, keeping 111573 word types\n",
            "2025-10-15 20:10:46,201 : INFO : PROGRESS: at sentence #640000, processed 14298046 words, keeping 112386 word types\n",
            "2025-10-15 20:10:46,231 : INFO : PROGRESS: at sentence #650000, processed 14522874 words, keeping 113151 word types\n",
            "2025-10-15 20:10:46,263 : INFO : PROGRESS: at sentence #660000, processed 14745445 words, keeping 113890 word types\n",
            "2025-10-15 20:10:46,293 : INFO : PROGRESS: at sentence #670000, processed 14970569 words, keeping 114613 word types\n",
            "2025-10-15 20:10:46,321 : INFO : PROGRESS: at sentence #680000, processed 15194625 words, keeping 115331 word types\n",
            "2025-10-15 20:10:46,350 : INFO : PROGRESS: at sentence #690000, processed 15416773 words, keeping 116099 word types\n",
            "2025-10-15 20:10:46,386 : INFO : PROGRESS: at sentence #700000, processed 15645695 words, keeping 116902 word types\n",
            "2025-10-15 20:10:46,422 : INFO : PROGRESS: at sentence #710000, processed 15865815 words, keeping 117541 word types\n",
            "2025-10-15 20:10:46,458 : INFO : PROGRESS: at sentence #720000, processed 16093342 words, keeping 118183 word types\n",
            "2025-10-15 20:10:46,490 : INFO : PROGRESS: at sentence #730000, processed 16316787 words, keeping 118912 word types\n",
            "2025-10-15 20:10:46,527 : INFO : PROGRESS: at sentence #740000, processed 16539147 words, keeping 119618 word types\n",
            "2025-10-15 20:10:46,562 : INFO : PROGRESS: at sentence #750000, processed 16758552 words, keeping 120264 word types\n",
            "2025-10-15 20:10:46,599 : INFO : PROGRESS: at sentence #760000, processed 16977111 words, keeping 120888 word types\n",
            "2025-10-15 20:10:46,636 : INFO : PROGRESS: at sentence #770000, processed 17203259 words, keeping 121656 word types\n",
            "2025-10-15 20:10:46,675 : INFO : PROGRESS: at sentence #780000, processed 17432844 words, keeping 122358 word types\n",
            "2025-10-15 20:10:46,706 : INFO : PROGRESS: at sentence #790000, processed 17660151 words, keeping 123033 word types\n",
            "2025-10-15 20:10:46,732 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 796172 sentences\n",
            "2025-10-15 20:10:46,732 : INFO : Creating a fresh vocabulary\n",
            "2025-10-15 20:10:46,789 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 16490 unique words (13.35% of original 123504, drops 107014)', 'datetime': '2025-10-15T20:10:46.789172', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
            "2025-10-15 20:10:46,789 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 17239125 word corpus (96.86% of original 17798270, drops 559145)', 'datetime': '2025-10-15T20:10:46.789762', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
            "2025-10-15 20:10:46,826 : INFO : deleting the raw counts dictionary of 123504 items\n",
            "2025-10-15 20:10:46,829 : INFO : sample=0.001 downsamples 48 most-common words\n",
            "2025-10-15 20:10:46,829 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12749798.434354488 word corpus (74.0%% of prior 17239125)', 'datetime': '2025-10-15T20:10:46.829929', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
            "2025-10-15 20:10:46,888 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
            "2025-10-15 20:10:46,888 : INFO : resetting layer weights\n",
            "2025-10-15 20:10:46,905 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-10-15T20:10:46.905302', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
            "2025-10-15 20:10:46,906 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2025-10-15T20:10:46.906015', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'train'}\n",
            "2025-10-15 20:10:47,913 : INFO : EPOCH 0 - PROGRESS: at 11.49% examples, 1456480 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:48,917 : INFO : EPOCH 0 - PROGRESS: at 21.01% examples, 1326117 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:49,920 : INFO : EPOCH 0 - PROGRESS: at 32.62% examples, 1373206 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:50,924 : INFO : EPOCH 0 - PROGRESS: at 44.32% examples, 1402078 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:51,927 : INFO : EPOCH 0 - PROGRESS: at 56.06% examples, 1421197 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:52,933 : INFO : EPOCH 0 - PROGRESS: at 67.86% examples, 1435381 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:53,936 : INFO : EPOCH 0 - PROGRESS: at 79.69% examples, 1445361 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:54,941 : INFO : EPOCH 0 - PROGRESS: at 91.49% examples, 1452285 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:55,661 : INFO : EPOCH 0: training on 17798270 raw words (12748737 effective words) took 8.7s, 1457102 effective words/s\n",
            "2025-10-15 20:10:56,666 : INFO : EPOCH 1 - PROGRESS: at 11.03% examples, 1398111 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:10:57,668 : INFO : EPOCH 1 - PROGRESS: at 21.12% examples, 1334182 words/s, in_qsize 8, out_qsize 0\n",
            "2025-10-15 20:10:58,672 : INFO : EPOCH 1 - PROGRESS: at 31.90% examples, 1343096 words/s, in_qsize 8, out_qsize 1\n",
            "2025-10-15 20:10:59,689 : INFO : EPOCH 1 - PROGRESS: at 41.09% examples, 1294722 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:00,691 : INFO : EPOCH 1 - PROGRESS: at 48.66% examples, 1230200 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:01,693 : INFO : EPOCH 1 - PROGRESS: at 57.85% examples, 1221432 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:02,695 : INFO : EPOCH 1 - PROGRESS: at 68.55% examples, 1241761 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:03,696 : INFO : EPOCH 1 - PROGRESS: at 79.63% examples, 1263298 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:04,696 : INFO : EPOCH 1 - PROGRESS: at 90.65% examples, 1279383 words/s, in_qsize 8, out_qsize 0\n",
            "2025-10-15 20:11:05,545 : INFO : EPOCH 1: training on 17798270 raw words (12749988 effective words) took 9.9s, 1290296 effective words/s\n",
            "2025-10-15 20:11:06,550 : INFO : EPOCH 2 - PROGRESS: at 10.52% examples, 1334299 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:07,561 : INFO : EPOCH 2 - PROGRESS: at 21.34% examples, 1343242 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:08,564 : INFO : EPOCH 2 - PROGRESS: at 32.51% examples, 1365352 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:09,570 : INFO : EPOCH 2 - PROGRESS: at 43.52% examples, 1374459 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:10,571 : INFO : EPOCH 2 - PROGRESS: at 54.76% examples, 1386598 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:11,572 : INFO : EPOCH 2 - PROGRESS: at 65.79% examples, 1391294 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:12,573 : INFO : EPOCH 2 - PROGRESS: at 76.88% examples, 1394579 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:13,575 : INFO : EPOCH 2 - PROGRESS: at 88.08% examples, 1398737 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:14,579 : INFO : EPOCH 2 - PROGRESS: at 99.33% examples, 1402450 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:14,636 : INFO : EPOCH 2: training on 17798270 raw words (12750604 effective words) took 9.1s, 1403033 effective words/s\n",
            "2025-10-15 20:11:15,646 : INFO : EPOCH 3 - PROGRESS: at 10.98% examples, 1383803 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:16,647 : INFO : EPOCH 3 - PROGRESS: at 22.03% examples, 1388878 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:17,649 : INFO : EPOCH 3 - PROGRESS: at 33.15% examples, 1393879 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:18,656 : INFO : EPOCH 3 - PROGRESS: at 44.15% examples, 1395157 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:19,656 : INFO : EPOCH 3 - PROGRESS: at 53.89% examples, 1364934 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:20,667 : INFO : EPOCH 3 - PROGRESS: at 63.67% examples, 1344765 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:21,678 : INFO : EPOCH 3 - PROGRESS: at 74.12% examples, 1341715 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:22,686 : INFO : EPOCH 3 - PROGRESS: at 83.62% examples, 1323935 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:23,693 : INFO : EPOCH 3 - PROGRESS: at 93.75% examples, 1319605 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:24,265 : INFO : EPOCH 3: training on 17798270 raw words (12749458 effective words) took 9.6s, 1324472 effective words/s\n",
            "2025-10-15 20:11:25,270 : INFO : EPOCH 4 - PROGRESS: at 10.87% examples, 1377199 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:26,283 : INFO : EPOCH 4 - PROGRESS: at 22.26% examples, 1398241 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:27,295 : INFO : EPOCH 4 - PROGRESS: at 33.61% examples, 1405050 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:28,297 : INFO : EPOCH 4 - PROGRESS: at 44.79% examples, 1410757 words/s, in_qsize 8, out_qsize 0\n",
            "2025-10-15 20:11:29,301 : INFO : EPOCH 4 - PROGRESS: at 55.94% examples, 1413544 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:30,301 : INFO : EPOCH 4 - PROGRESS: at 67.06% examples, 1416254 words/s, in_qsize 8, out_qsize 0\n",
            "2025-10-15 20:11:31,302 : INFO : EPOCH 4 - PROGRESS: at 77.84% examples, 1409931 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:32,306 : INFO : EPOCH 4 - PROGRESS: at 88.52% examples, 1403894 words/s, in_qsize 7, out_qsize 0\n",
            "2025-10-15 20:11:33,314 : INFO : EPOCH 4 - PROGRESS: at 99.72% examples, 1405497 words/s, in_qsize 5, out_qsize 0\n",
            "2025-10-15 20:11:33,331 : INFO : EPOCH 4: training on 17798270 raw words (12750205 effective words) took 9.1s, 1406774 effective words/s\n",
            "2025-10-15 20:11:33,332 : INFO : Word2Vec lifecycle event {'msg': 'training on 88991350 raw words (63748992 effective words) took 46.4s, 1373135 effective words/s', 'datetime': '2025-10-15T20:11:33.332208', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'train'}\n",
            "2025-10-15 20:11:33,332 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=16490, vector_size=300, alpha=0.025>', 'datetime': '2025-10-15T20:11:33.332698', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'created'}\n",
            "/var/folders/z5/5fqf9f4x2f9ddt98nd465jh00000gn/T/ipykernel_38956/110562228.py:23: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "2025-10-15 20:11:33,337 : WARNING : destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
            "2025-10-15 20:11:33,339 : INFO : Word2Vec lifecycle event {'fname_or_handle': '300features_40minwords_10context', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-10-15T20:11:33.339178', 'gensim': '4.3.3', 'python': '3.9.12 (main, Apr  5 2022, 01:52:34) \\n[Clang 12.0.0 ]', 'platform': 'macOS-26.0.1-arm64-arm-64bit', 'event': 'saving'}\n",
            "2025-10-15 20:11:33,339 : INFO : not storing attribute cum_table\n",
            "2025-10-15 20:11:33,361 : INFO : saved 300features_40minwords_10context\n"
          ]
        }
      ],
      "source": [
        "# Import the built-in logging module and configure it so that Word2Vec\n",
        "# creates nice output messages\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 300    # Word vector dimensionality\n",
        "min_word_count = 40   # Minimum word count\n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
        "            vector_size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling)\n",
        "\n",
        "# If you don't plan to train the model any further, calling\n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and\n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BMUWwQAGoIQO",
        "outputId": "4cbba9cf-572c-4930-d227-ce212303e2db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'kitchen'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.doesnt_match(\"man woman child kitchen\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-o_dwUesoISb",
        "outputId": "fc181831-bdad-4918-c35c-4b5b93a728ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'berlin'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.doesnt_match(\"france england germany berlin\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Wq5cy_E2oIVI",
        "outputId": "764dd77d-adbd-48a4-aca9-eff7d27aeb64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'austria'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.doesnt_match(\"paris berlin london austria\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhIvaSVpuJmh",
        "outputId": "d87540ff-8926-4be9-f9d8-7b6937a4a552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('woman', 0.6145219802856445),\n",
              " ('lad', 0.5886115431785583),\n",
              " ('lady', 0.5732795000076294),\n",
              " ('millionaire', 0.5231574773788452),\n",
              " ('guy', 0.5225004553794861),\n",
              " ('monk', 0.5199234485626221),\n",
              " ('businessman', 0.5100327730178833),\n",
              " ('farmer', 0.5015870928764343),\n",
              " ('soldier', 0.5012238025665283),\n",
              " ('men', 0.5008193254470825)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(\"man\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odKaZxfLucti",
        "outputId": "3834651f-26bf-4a52-a5bd-d62481045371"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('princess', 0.6551982164382935),\n",
              " ('victoria', 0.6020163297653198),\n",
              " ('bride', 0.5963108539581299),\n",
              " ('belle', 0.5918349623680115),\n",
              " ('mistress', 0.5826388001441956),\n",
              " ('maid', 0.582522988319397),\n",
              " ('showgirl', 0.5799010396003723),\n",
              " ('stepmother', 0.5652430057525635),\n",
              " ('latifah', 0.5630404949188232),\n",
              " ('prince', 0.5611833333969116)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(\"queen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpVPWcL_utw2",
        "outputId": "3aa29f28-f9d9-489e-c742-87c88ee9026c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('terrible', 0.7683419585227966),\n",
              " ('atrocious', 0.7360451221466064),\n",
              " ('horrible', 0.7290331125259399),\n",
              " ('dreadful', 0.6988271474838257),\n",
              " ('abysmal', 0.6955971121788025),\n",
              " ('horrendous', 0.6760333776473999),\n",
              " ('appalling', 0.675796627998352),\n",
              " ('horrid', 0.6715326905250549),\n",
              " ('amateurish', 0.6361018419265747),\n",
              " ('lousy', 0.6212121248245239)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(\"awful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLzY5EzJu0AS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO1/vjtJHsP8GV4Y3qaRL3n",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
